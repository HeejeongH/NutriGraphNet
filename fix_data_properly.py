#!/usr/bin/env python3
"""
ë°ì´í„° ì •ê·œí™” ì¬ì ìš©
- ì›ë³¸ ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì •ê·œí™”
"""

import torch
import pickle
import numpy as np

def quantile_normalize(weights):
    """Quantile ê¸°ë°˜ ì •ê·œí™” - ìƒìœ„ ë°ì´í„° ê°•ì¡°"""
    if isinstance(weights, torch.Tensor):
        weights_np = weights.cpu().numpy()
    else:
        weights_np = weights
    
    # ìƒìœ„ 10%ë¥¼ 1.0 ê·¼ì²˜ë¡œ, ë‚˜ë¨¸ì§€ëŠ” 0-0.9 ë²”ìœ„ë¡œ
    q90 = np.quantile(weights_np, 0.9)
    
    normalized = np.where(
        weights_np >= q90,
        0.9 + 0.1 * (weights_np - q90) / (weights_np.max() - q90 + 1e-8),
        0.9 * (weights_np / (q90 + 1e-8))
    )
    
    normalized = np.clip(normalized, 0, 1)
    return torch.tensor(normalized, dtype=torch.float32)


def minmax_normalize(weights):
    """MinMax ì •ê·œí™” - ì „ì²´ ë²”ìœ„ë¥¼ 0-1ë¡œ"""
    if isinstance(weights, torch.Tensor):
        weights_np = weights.cpu().numpy()
    else:
        weights_np = weights
    
    if weights_np.max() > weights_np.min():
        normalized = (weights_np - weights_np.min()) / (weights_np.max() - weights_np.min())
    else:
        normalized = np.ones_like(weights_np) * 0.5
    
    return torch.tensor(normalized, dtype=torch.float32)


print("\n" + "="*70)
print("ğŸ”§ ë°ì´í„° ì •ê·œí™” ì¬ì ìš©")
print("="*70)

# Load original data
print("\nğŸ“‚ Loading original data...")
with open('etc/old_versions/processed_data_GNN_cpu.pkl', 'rb') as f:
    data = pickle.load(f)

print("âœ… Data loaded")

# Check original weights
eats_weight = data['user', 'eats', 'food'].edge_attr
print(f"\nğŸ“Š Original User-eats-Food weights:")
print(f"   Min: {eats_weight.min():.6f}")
print(f"   Max: {eats_weight.max():.6f}")
print(f"   Mean: {eats_weight.mean():.6f}")
print(f"   Median: {eats_weight.median():.6f}")

# Apply MinMax normalization (simpler, better for training)
print(f"\nğŸ”§ Applying MinMax normalization...")
normalized_eats = minmax_normalize(eats_weight)

print(f"\nğŸ“Š Normalized User-eats-Food weights:")
print(f"   Min: {normalized_eats.min():.6f}")
print(f"   Max: {normalized_eats.max():.6f}")
print(f"   Mean: {normalized_eats.mean():.6f}")
print(f"   Median: {normalized_eats.median():.6f}")

# Apply to data
data['user', 'eats', 'food'].edge_attr = normalized_eats
data['food', 'rev_eats', 'user'].edge_attr = normalized_eats

# Normalize food-contains-ingredient
contains_weight = data['food', 'contains', 'ingredient'].edge_attr
normalized_contains = minmax_normalize(contains_weight)
data['food', 'contains', 'ingredient'].edge_attr = normalized_contains
data['ingredient', 'rev_contains', 'food'].edge_attr = normalized_contains

print(f"\nğŸ“Š Normalized Food-contains-Ingredient weights:")
print(f"   Min: {normalized_contains.min():.6f}")
print(f"   Max: {normalized_contains.max():.6f}")
print(f"   Mean: {normalized_contains.mean():.6f}")

# Save
output_path = "data/processed_data/processed_data_GNN_v3.pkl"
print(f"\nğŸ’¾ Saving to: {output_path}")
with open(output_path, 'wb') as f:
    pickle.dump(data, f)

import os
file_size = os.path.getsize(output_path) / (1024 * 1024)
print(f"âœ… Saved! File size: {file_size:.2f} MB")

print("\n" + "="*70)
print("âœ… ì •ê·œí™” ì™„ë£Œ!")
print("="*70)
print(f"\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
print(f"  python train_v2.py --data_path {output_path} --model graphsage --epochs 10")
