"""
ê¸°ì¡´ processed_data_GNN_cpu.pklì˜ ë¬¸ì œì  ìˆ˜ì •
- Edge weight ì •ê·œí™”
- Edge ì´ë¦„ ìˆ˜ì •
- Health score ê²€ì¦
"""

import torch
import pickle
import numpy as np
from pathlib import Path
import sys
import os

# Health score calculator import
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))


def normalize_weights(weights, method='quantile'):
    """Edge weight ì •ê·œí™”
    
    Args:
        weights: ì›ë³¸ ê°€ì¤‘ì¹˜
        method: ì •ê·œí™” ë°©ë²•
            - 'quantile': ìƒìœ„ 10%ë¥¼ 1.0ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ëŠ” 0-0.9 ë²”ìœ„ë¡œ ë§¤í•‘ (ê¶Œìž¥)
            - 'minmax': ì „ì²´ ë²”ìœ„ë¥¼ 0-1ë¡œ ì„ í˜• ë§¤í•‘
            - 'log1p': Log ë³€í™˜ í›„ ì •ê·œí™” (ìž‘ì€ ê°’ ê°•ì¡°, ê¶Œìž¥ ì•ˆ í•¨)
            - 'clip': ì´ìƒì¹˜ ì œê±° í›„ ì •ê·œí™”
    """
    if isinstance(weights, torch.Tensor):
        weights = weights.cpu().numpy()
    
    if method == 'quantile':
        # ìƒìœ„ 10%ë¥¼ ê°•ì¡°í•˜ëŠ” ì •ê·œí™”
        q90 = np.quantile(weights, 0.9)
        normalized = np.where(
            weights >= q90,
            0.9 + 0.1 * (weights - q90) / (weights.max() - q90 + 1e-8),
            0.9 * (weights / (q90 + 1e-8))
        )
        normalized = np.clip(normalized, 0, 1)
    elif method == 'minmax':
        # Min-max normalization
        if weights.max() > weights.min():
            normalized = (weights - weights.min()) / (weights.max() - weights.min())
        else:
            normalized = np.ones_like(weights) * 0.5
    elif method == 'log1p':
        # Log transformation: log(1 + x) - ìž‘ì€ ê°’ ê°•ì¡°
        normalized = np.log1p(weights)
        if normalized.max() > 0:
            normalized = normalized / normalized.max()
    elif method == 'clip':
        # Clipping outliers
        normalized = np.clip(weights, 0, 10)
        if normalized.max() > 0:
            normalized = normalized / 10.0
    else:
        normalized = weights
    
    return torch.tensor(normalized, dtype=torch.float32)


def fix_data(input_path, output_path):
    """ë°ì´í„° ìˆ˜ì •"""
    
    print(f"\n{'='*70}")
    print(f"ðŸ”§ Fixing Existing Graph Data")
    print(f"{'='*70}")
    
    # ë°ì´í„° ë¡œë“œ
    print(f"\nðŸ“‚ Loading data from: {input_path}")
    with open(input_path, 'rb') as f:
        data = pickle.load(f)
    
    print(f"âœ… Data loaded")
    
    # í˜„ìž¬ ìƒíƒœ í™•ì¸
    print(f"\n{'='*70}")
    print("ðŸ“Š Current Data Status")
    print(f"{'='*70}")
    
    print(f"\nâœ… Nodes:")
    for node_type in data.node_types:
        print(f"   {node_type}: {data[node_type].num_nodes:,}")
    
    print(f"\nâœ… Edges:")
    for edge_type in data.edge_types:
        edge_count = data[edge_type].edge_index.shape[1]
        has_attr = hasattr(data[edge_type], 'edge_attr') and data[edge_type].edge_attr is not None
        
        if has_attr:
            attr = data[edge_type].edge_attr
            print(f"   {edge_type}: {edge_count:,} edges, weights [{attr.min():.4f}, {attr.max():.4f}]")
        else:
            print(f"   {edge_type}: {edge_count:,} edges")
    
    # ìˆ˜ì • ìž‘ì—…
    print(f"\n{'='*70}")
    print("ðŸ”§ Applying Fixes")
    print(f"{'='*70}")
    
    # 1. User-eats-Food edge weight ì •ê·œí™”
    if ('user', 'eats', 'food') in data.edge_types:
        print(f"\n1ï¸âƒ£ Normalizing User-eats-Food weights...")
        old_weights = data[('user', 'eats', 'food')].edge_attr
        
        if old_weights is not None:
            print(f"   Before: [{old_weights.min():.4f}, {old_weights.max():.4f}], mean={old_weights.mean():.4f}")
            
            new_weights = normalize_weights(old_weights, method='log1p')
            data[('user', 'eats', 'food')].edge_attr = new_weights
            
            print(f"   After:  [{new_weights.min():.4f}, {new_weights.max():.4f}], mean={new_weights.mean():.4f}")
    
    # 2. Food-rev_eats-User edge weightë„ ë™ì¼í•˜ê²Œ
    if ('food', 'rev_eats', 'user') in data.edge_types:
        print(f"\n2ï¸âƒ£ Normalizing Food-rev_eats-User weights...")
        old_weights = data[('food', 'rev_eats', 'user')].edge_attr
        
        if old_weights is not None:
            new_weights = normalize_weights(old_weights, method='log1p')
            data[('food', 'rev_eats', 'user')].edge_attr = new_weights
            print(f"   Updated: [{new_weights.min():.4f}, {new_weights.max():.4f}]")
    
    # 3. Food-contains-Ingredient edge weight ì •ê·œí™”
    if ('food', 'contains', 'ingredient') in data.edge_types:
        print(f"\n3ï¸âƒ£ Normalizing Food-contains-Ingredient weights...")
        old_weights = data[('food', 'contains', 'ingredient')].edge_attr
        
        if old_weights is not None:
            print(f"   Before: [{old_weights.min():.4f}, {old_weights.max():.4f}]")
            
            new_weights = normalize_weights(old_weights, method='log1p')
            data[('food', 'contains', 'ingredient')].edge_attr = new_weights
            
            print(f"   After:  [{new_weights.min():.4f}, {new_weights.max():.4f}]")
    
    # 4. Ingredient-rev_contains-Foodë„ ë™ì¼í•˜ê²Œ
    if ('ingredient', 'rev_contains', 'food') in data.edge_types:
        print(f"\n4ï¸âƒ£ Normalizing Ingredient-rev_contains-Food weights...")
        old_weights = data[('ingredient', 'rev_contains', 'food')].edge_attr
        
        if old_weights is not None:
            new_weights = normalize_weights(old_weights, method='log1p')
            data[('ingredient', 'rev_contains', 'food')].edge_attr = new_weights
            print(f"   Updated: [{new_weights.min():.4f}, {new_weights.max():.4f}]")
    
    # 5. Edge ì´ë¦„ ë³€ê²½: pairs -> similar (ìžˆìœ¼ë©´)
    if ('food', 'pairs', 'food') in data.edge_types:
        print(f"\n5ï¸âƒ£ Renaming Food-pairs-Food to Food-similar-Food...")
        
        # ë°ì´í„° ë³µì‚¬
        data['food', 'similar', 'food'].edge_index = data[('food', 'pairs', 'food')].edge_index
        if hasattr(data[('food', 'pairs', 'food')], 'edge_attr'):
            data['food', 'similar', 'food'].edge_attr = data[('food', 'pairs', 'food')].edge_attr
        
        # ê¸°ì¡´ ì‚­ì œ (ì´ê±´ ì§ì ‘ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ë¬´ì‹œ)
        print(f"   âš ï¸  Cannot delete old 'pairs' edge (PyG limitation)")
        print(f"   âœ… Added 'similar' edge with same data")
    
    # 6. Health score í™•ì¸
    if ('user', 'healthness', 'food') in data.edge_types:
        print(f"\n6ï¸âƒ£ Checking Health scores...")
        health_scores = data[('user', 'healthness', 'food')].edge_attr
        
        if health_scores is not None:
            print(f"   Health scores: [{health_scores.min():.4f}, {health_scores.max():.4f}]")
            print(f"   Mean: {health_scores.mean():.4f}, Median: {health_scores.median():.4f}")
            
            # ì´ë¯¸ 0-1 ë²”ìœ„ë©´ OK
            if health_scores.min() >= 0 and health_scores.max() <= 1.0:
                print(f"   âœ… Health scores are already normalized")
            else:
                print(f"   âš ï¸  Health scores may need recalculation")
    
    # ìµœì¢… ìš”ì•½
    print(f"\n{'='*70}")
    print("ðŸ“Š Fixed Data Summary")
    print(f"{'='*70}")
    
    print(f"\nâœ… Edges:")
    for edge_type in data.edge_types:
        edge_count = data[edge_type].edge_index.shape[1]
        has_attr = hasattr(data[edge_type], 'edge_attr') and data[edge_type].edge_attr is not None
        
        if has_attr:
            attr = data[edge_type].edge_attr
            print(f"   {edge_type}: {edge_count:,} edges, weights [{attr.min():.4f}, {attr.max():.4f}]")
        else:
            print(f"   {edge_type}: {edge_count:,} edges")
    
    # ì €ìž¥
    print(f"\nðŸ’¾ Saving fixed data to: {output_path}")
    
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    with open(output_path, 'wb') as f:
        pickle.dump(data, f)
    
    print(f"âœ… Data saved successfully!")
    
    file_size = os.path.getsize(output_path) / 1024 / 1024
    print(f"\nFile size: {file_size:.2f} MB")
    
    print(f"\n{'='*70}")
    print("ðŸŽ‰ All fixes applied!")
    print(f"{'='*70}\n")
    
    return data


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Fix existing graph data')
    parser.add_argument('--input', type=str, 
                       default='./processed_data/processed_data_GNN_cpu.pkl',
                       help='Input pickle file path')
    parser.add_argument('--output', type=str, 
                       default='./processed_data/processed_data_GNN_fixed.pkl',
                       help='Output pickle file path')
    
    args = parser.parse_args()
    
    fix_data(args.input, args.output)
