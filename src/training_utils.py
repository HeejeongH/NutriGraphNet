"""
Training Utilities
ë…¼ë¬¸ì˜ ìµœì í™” ê¸°ë²• êµ¬í˜„: Cosine Annealing, Early Stopping
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Optional, Dict, Any


class CosineAnnealingWithWarmRestarts:
    """
    ë…¼ë¬¸ì˜ Cosine Annealing with Warm Restarts êµ¬í˜„
    
    Î·_t = Î·_min + 0.5(Î·_max - Î·_min)(1 + cos(T_cur/T_i Ã— Ï€))
    """
    
    def __init__(self, optimizer, T_0=10, T_mult=2, eta_min=1e-6, eta_max=0.001, last_epoch=-1):
        """
        Args:
            optimizer: PyTorch optimizer
            T_0: ì²« ë²ˆì§¸ ì¬ì‹œì‘ê¹Œì§€ì˜ ì—í¬í¬ ìˆ˜
            T_mult: ì¬ì‹œì‘ ì£¼ê¸° ë°°ìœ¨
            eta_min: ìµœì†Œ í•™ìŠµë¥ 
            eta_max: ìµœëŒ€ í•™ìŠµë¥ 
            last_epoch: ë§ˆì§€ë§‰ ì—í¬í¬ ë²ˆí˜¸
        """
        self.optimizer = optimizer
        self.T_0 = T_0
        self.T_mult = T_mult
        self.eta_min = eta_min
        self.eta_max = eta_max
        self.T_cur = last_epoch + 1
        self.T_i = T_0
        self.epoch = last_epoch + 1
        
        # ì´ˆê¸° í•™ìŠµë¥  ì„¤ì •
        self._set_lr(eta_max)
    
    def _set_lr(self, lr):
        """í•™ìŠµë¥  ì„¤ì •"""
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
    
    def step(self):
        """í•œ ìŠ¤í… ì§„í–‰"""
        self.T_cur += 1
        
        # Restart ì²´í¬
        if self.T_cur >= self.T_i:
            self.T_cur = 0
            self.T_i = self.T_i * self.T_mult
        
        # Cosine annealing ê³„ì‚°
        lr = self.eta_min + 0.5 * (self.eta_max - self.eta_min) * (
            1 + np.cos(np.pi * self.T_cur / self.T_i)
        )
        
        self._set_lr(lr)
        self.epoch += 1
        
        return lr
    
    def get_last_lr(self):
        """í˜„ì¬ í•™ìŠµë¥  ë°˜í™˜"""
        return [group['lr'] for group in self.optimizer.param_groups]
    
    def state_dict(self):
        """ìƒíƒœ ì €ì¥"""
        return {
            'T_0': self.T_0,
            'T_mult': self.T_mult,
            'eta_min': self.eta_min,
            'eta_max': self.eta_max,
            'T_cur': self.T_cur,
            'T_i': self.T_i,
            'epoch': self.epoch
        }
    
    def load_state_dict(self, state_dict):
        """ìƒíƒœ ë¡œë“œ"""
        self.T_0 = state_dict['T_0']
        self.T_mult = state_dict['T_mult']
        self.eta_min = state_dict['eta_min']
        self.eta_max = state_dict['eta_max']
        self.T_cur = state_dict['T_cur']
        self.T_i = state_dict['T_i']
        self.epoch = state_dict['epoch']


class EarlyStopping:
    """
    ë…¼ë¬¸ì˜ Early Stopping with Adaptive Patience êµ¬í˜„
    """
    
    def __init__(self, patience=10, min_delta=0.0, mode='max', verbose=True):
        """
        Args:
            patience: ê°œì„ ì´ ì—†ì–´ë„ ê¸°ë‹¤ë¦´ ì—í¬í¬ ìˆ˜
            min_delta: ê°œì„ ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ë³€í™”ëŸ‰
            mode: 'max' (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ) or 'min' (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            verbose: ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.verbose = verbose
        
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_epoch = 0
        
        # Modeì— ë”°ë¥¸ ë¹„êµ í•¨ìˆ˜ ì„¤ì •
        if mode == 'max':
            self.is_better = lambda new, best: new > best + min_delta
            self.best_score = -np.inf
        else:
            self.is_better = lambda new, best: new < best - min_delta
            self.best_score = np.inf
    
    def __call__(self, score, epoch, model=None):
        """
        Early stopping ì²´í¬
        
        Args:
            score: í˜„ì¬ ì ìˆ˜ (validation metric)
            epoch: í˜„ì¬ ì—í¬í¬
            model: ì €ì¥í•  ëª¨ë¸ (optional)
            
        Returns:
            bool: True if should stop training
        """
        if self.is_better(score, self.best_score):
            # ê°œì„ ë¨
            self.best_score = score
            self.best_epoch = epoch
            self.counter = 0
            
            if self.verbose:
                print(f"  âœ“ Validation improved to {score:.4f} at epoch {epoch}")
            
            # ëª¨ë¸ ì €ì¥ (optional)
            if model is not None:
                self.best_model_state = model.state_dict()
            
            return False
        else:
            # ê°œì„  ì—†ìŒ
            self.counter += 1
            
            if self.verbose and self.counter >= self.patience // 2:
                print(f"  âš  No improvement for {self.counter} epochs (patience: {self.patience})")
            
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f"  ğŸ›‘ Early stopping triggered at epoch {epoch}")
                    print(f"     Best score: {self.best_score:.4f} at epoch {self.best_epoch}")
                return True
            
            return False
    
    def reset(self):
        """ìƒíƒœ ì´ˆê¸°í™”"""
        self.counter = 0
        self.early_stop = False
        if self.mode == 'max':
            self.best_score = -np.inf
        else:
            self.best_score = np.inf
        self.best_epoch = 0


class TrainingMonitor:
    """
    í›ˆë ¨ ê³¼ì • ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­ ê¸°ë¡
    """
    
    def __init__(self):
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_metrics': [],
            'val_metrics': [],
            'learning_rates': []
        }
    
    def log_epoch(self, epoch, train_loss, val_loss=None, train_metrics=None, 
                  val_metrics=None, lr=None):
        """ì—í¬í¬ë³„ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.history['train_loss'].append(train_loss)
        
        if val_loss is not None:
            self.history['val_loss'].append(val_loss)
        
        if train_metrics is not None:
            self.history['train_metrics'].append(train_metrics)
        
        if val_metrics is not None:
            self.history['val_metrics'].append(val_metrics)
        
        if lr is not None:
            self.history['learning_rates'].append(lr)
    
    def get_best_epoch(self, metric='val_loss', mode='min'):
        """ìµœê³  ì„±ëŠ¥ ì—í¬í¬ ì°¾ê¸°"""
        if metric not in self.history or len(self.history[metric]) == 0:
            return 0
        
        values = self.history[metric]
        if mode == 'min':
            best_idx = np.argmin(values)
        else:
            best_idx = np.argmax(values)
        
        return best_idx
    
    def print_summary(self):
        """í›ˆë ¨ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š Training Summary")
        print("="*60)
        
        if len(self.history['train_loss']) > 0:
            print(f"Final Train Loss: {self.history['train_loss'][-1]:.4f}")
            print(f"Best Train Loss:  {min(self.history['train_loss']):.4f}")
        
        if len(self.history['val_loss']) > 0:
            best_val_idx = np.argmin(self.history['val_loss'])
            print(f"Final Val Loss:   {self.history['val_loss'][-1]:.4f}")
            print(f"Best Val Loss:    {self.history['val_loss'][best_val_idx]:.4f} (epoch {best_val_idx+1})")
        
        if len(self.history['learning_rates']) > 0:
            print(f"Final LR:         {self.history['learning_rates'][-1]:.2e}")
        
        print("="*60)


def compute_metrics(predictions, targets, threshold=0.5):
    """
    ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚°
    
    Args:
        predictions: ì˜ˆì¸¡ í™•ë¥  (0-1)
        targets: ì‹¤ì œ ë ˆì´ë¸” (0 or 1)
        threshold: ë¶„ë¥˜ ì„ê³„ê°’
        
    Returns:
        dict: ê³„ì‚°ëœ ë©”íŠ¸ë¦­ë“¤
    """
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, 
        f1_score, roc_auc_score
    )
    
    # Numpyë¡œ ë³€í™˜
    if torch.is_tensor(predictions):
        predictions = predictions.detach().cpu().numpy()
    if torch.is_tensor(targets):
        targets = targets.detach().cpu().numpy()
    
    # ì´ì§„ ì˜ˆì¸¡
    binary_preds = (predictions > threshold).astype(int)
    
    metrics = {
        'accuracy': accuracy_score(targets, binary_preds),
        'precision': precision_score(targets, binary_preds, zero_division=0),
        'recall': recall_score(targets, binary_preds, zero_division=0),
        'f1': f1_score(targets, binary_preds, zero_division=0),
        'auc': roc_auc_score(targets, predictions) if len(np.unique(targets)) > 1 else 0.5
    }
    
    return metrics


class GradientClipper:
    """
    ë…¼ë¬¸ì˜ Gradient Clipping êµ¬í˜„
    """
    
    def __init__(self, max_norm=1.0, norm_type=2):
        """
        Args:
            max_norm: ìµœëŒ€ ê·¸ë˜ë””ì–¸íŠ¸ norm
            norm_type: norm íƒ€ì… (2 = L2 norm)
        """
        self.max_norm = max_norm
        self.norm_type = norm_type
    
    def clip(self, model):
        """ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘"""
        return torch.nn.utils.clip_grad_norm_(
            model.parameters(), 
            max_norm=self.max_norm, 
            norm_type=self.norm_type
        )


def get_optimizer_with_scheduler(model, config):
    """
    ë…¼ë¬¸ì˜ ìµœì í™” ì„¤ì • ìƒì„±
    
    Args:
        model: PyTorch ëª¨ë¸
        config: dict with keys: lr, weight_decay, scheduler_type, etc.
        
    Returns:
        tuple: (optimizer, scheduler)
    """
    # AdamW optimizer (ë…¼ë¬¸ ê¶Œì¥)
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.get('lr', 0.001),
        weight_decay=config.get('weight_decay', 0.01)
    )
    
    # Scheduler ì„¤ì •
    scheduler_type = config.get('scheduler_type', 'cosine_warmup')
    
    if scheduler_type == 'cosine_warmup':
        scheduler = CosineAnnealingWithWarmRestarts(
            optimizer,
            T_0=config.get('T_0', 10),
            T_mult=config.get('T_mult', 2),
            eta_min=config.get('eta_min', 1e-6),
            eta_max=config.get('lr', 0.001)
        )
    elif scheduler_type == 'cosine':
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=config.get('T_max', 50),
            eta_min=config.get('eta_min', 1e-6)
        )
    elif scheduler_type == 'step':
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer,
            step_size=config.get('step_size', 10),
            gamma=config.get('gamma', 0.5)
        )
    else:
        scheduler = None
    
    return optimizer, scheduler


def save_checkpoint(model, optimizer, scheduler, epoch, metrics, filepath):
    """
    ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    
    Args:
        model: PyTorch ëª¨ë¸
        optimizer: Optimizer
        scheduler: Learning rate scheduler
        epoch: í˜„ì¬ ì—í¬í¬
        metrics: ë©”íŠ¸ë¦­ dict
        filepath: ì €ì¥ ê²½ë¡œ
    """
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics
    }
    
    if scheduler is not None:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()
    
    torch.save(checkpoint, filepath)
    print(f"âœ… Checkpoint saved to {filepath}")


def load_checkpoint(model, optimizer, scheduler, filepath, device='cpu'):
    """
    ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    
    Args:
        model: PyTorch ëª¨ë¸
        optimizer: Optimizer
        scheduler: Learning rate scheduler
        filepath: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        device: ë””ë°”ì´ìŠ¤
        
    Returns:
        dict: {'epoch', 'metrics'}
    """
    checkpoint = torch.load(filepath, map_location=device)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    if scheduler is not None and 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    print(f"âœ… Checkpoint loaded from {filepath}")
    print(f"   Epoch: {checkpoint['epoch']}")
    print(f"   Metrics: {checkpoint.get('metrics', {})}")
    
    return {
        'epoch': checkpoint['epoch'],
        'metrics': checkpoint.get('metrics', {})
    }
