"""
Health-aware í‰ê°€ ë©”íŠ¸ë¦­
ì—°êµ¬ ëª©ì : ì„ í˜¸ë„ ì˜ˆì¸¡ + ê±´ê°•ë„ ê³ ë ¤ë¥¼ ëª¨ë‘ í‰ê°€
"""

import torch
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score


def compute_comprehensive_metrics(predictions, targets, health_scores, 
                                   pred_threshold=0.5, health_threshold=0.6):
    """
    ì¢…í•© í‰ê°€ ë©”íŠ¸ë¦­
    
    Args:
        predictions: ëª¨ë¸ì˜ ì¶”ì²œ í™•ë¥  (0-1)
        targets: ì‹¤ì œ ì„ í˜¸ë„ ë ˆì´ë¸” (0 or 1)
        health_scores: ìŒì‹ì˜ ê±´ê°• ì ìˆ˜ (0-1)
        pred_threshold: ì¶”ì²œ íŒë‹¨ threshold
        health_threshold: ê±´ê°•ì‹ íŒë‹¨ threshold
        
    Returns:
        dict: ì„ í˜¸ë„ + ê±´ê°•ë„ ë©”íŠ¸ë¦­
    """
    
    # Numpy ë³€í™˜
    predictions = predictions.cpu().detach().numpy()
    targets = targets.cpu().detach().numpy()
    health_scores = health_scores.cpu().detach().numpy()
    
    # Binary predictions
    pred_binary = (predictions > pred_threshold).astype(int)
    
    # ============================================
    # 1. ê¸°ë³¸ ì„ í˜¸ë„ ì˜ˆì¸¡ ë©”íŠ¸ë¦­
    # ============================================
    preference_metrics = {
        'accuracy': accuracy_score(targets, pred_binary),
        'precision': precision_score(targets, pred_binary, zero_division=0),
        'recall': recall_score(targets, pred_binary, zero_division=0),
        'f1': f1_score(targets, pred_binary, zero_division=0),
        'auc': roc_auc_score(targets, predictions) if len(np.unique(targets)) > 1 else 0.5
    }
    
    # ============================================
    # 2. ê±´ê°•ë„ ê³ ë ¤ ë©”íŠ¸ë¦­ (í•µì‹¬!)
    # ============================================
    
    # ì¶”ì²œëœ ìŒì‹ë“¤ì˜ í‰ê·  ê±´ê°• ì ìˆ˜
    recommended_indices = np.where(pred_binary == 1)[0]
    
    if len(recommended_indices) > 0:
        avg_health_of_recommendations = health_scores[recommended_indices].mean()
        
        # ê±´ê°•ì‹ ì¶”ì²œ ë¹„ìœ¨ (ì¶”ì²œëœ ê²ƒ ì¤‘ ê±´ê°•ì‹ ë¹„ìœ¨)
        healthy_food_mask = health_scores > health_threshold
        healthy_recommendations = np.sum(pred_binary * healthy_food_mask)
        health_precision = healthy_recommendations / len(recommended_indices)
    else:
        avg_health_of_recommendations = 0.0
        health_precision = 0.0
    
    # ì‹¤ì œë¡œ ì„ í˜¸í•˜ëŠ” ìŒì‹ë“¤ì˜ ê±´ê°• ì ìˆ˜
    preferred_indices = np.where(targets == 1)[0]
    if len(preferred_indices) > 0:
        avg_health_of_preferences = health_scores[preferred_indices].mean()
    else:
        avg_health_of_preferences = 0.0
    
    health_metrics = {
        'avg_health_score': avg_health_of_recommendations,  # ì¶”ì²œ ìŒì‹ì˜ í‰ê·  ê±´ê°•ë„
        'health_precision': health_precision,  # ê±´ê°•ì‹ ì¶”ì²œ ì •ë°€ë„
        'health_improvement': avg_health_of_recommendations - avg_health_of_preferences  # ê±´ê°•ë„ í–¥ìƒ
    }
    
    # ============================================
    # 3. ì„ í˜¸ë„-ê±´ê°•ë„ ê· í˜• ë©”íŠ¸ë¦­
    # ============================================
    
    # F1 scoreì™€ Health scoreì˜ ì¡°í™” í‰ê·  (Health-aware F1)
    if preference_metrics['f1'] > 0 and health_metrics['avg_health_score'] > 0:
        health_aware_f1 = 2 * (preference_metrics['f1'] * health_metrics['avg_health_score']) / \
                         (preference_metrics['f1'] + health_metrics['avg_health_score'])
    else:
        health_aware_f1 = 0.0
    
    # Top-K ì¶”ì²œì˜ ê±´ê°•ë„ (ìƒìœ„ 10ê°œ ì¶”ì²œ)
    top_k = min(10, len(predictions))
    top_k_indices = np.argsort(predictions)[-top_k:]
    top_k_health = health_scores[top_k_indices].mean()
    
    # Top-Kì˜ ì •í™•ë„
    top_k_accuracy = targets[top_k_indices].mean()
    
    balance_metrics = {
        'health_aware_f1': health_aware_f1,  # ì„ í˜¸ë„-ê±´ê°•ë„ ì¡°í™” ë©”íŠ¸ë¦­
        'top_k_health': top_k_health,  # Top-K ì¶”ì²œì˜ ê±´ê°•ë„
        'top_k_accuracy': top_k_accuracy  # Top-K ì¶”ì²œì˜ ì •í™•ë„
    }
    
    # ============================================
    # 4. Health-aware Recall (ì¤‘ìš”!)
    # ============================================
    # "ê±´ê°•í•œ ìŒì‹ ì¤‘ì—ì„œ ì–¼ë§ˆë‚˜ ë§ì´ ì¶”ì²œí–ˆëŠ”ê°€?"
    healthy_food_indices = np.where(health_scores > health_threshold)[0]
    
    if len(healthy_food_indices) > 0:
        healthy_recommendations_count = np.sum(pred_binary[healthy_food_indices])
        health_aware_recall = healthy_recommendations_count / len(healthy_food_indices)
    else:
        health_aware_recall = 0.0
    
    health_metrics['health_aware_recall'] = health_aware_recall
    
    # í†µí•© ê²°ê³¼
    return {
        **preference_metrics,
        **health_metrics,
        **balance_metrics
    }


def print_metrics_comparison(baseline_metrics, health_aware_metrics, model_names=None):
    """
    Baseline vs Health-aware ëª¨ë¸ ë¹„êµ ì¶œë ¥
    
    Args:
        baseline_metrics: Baseline ëª¨ë¸ ë©”íŠ¸ë¦­ dict
        health_aware_metrics: Health-aware ëª¨ë¸ ë©”íŠ¸ë¦­ dict
        model_names: (baseline_name, health_aware_name) tuple
    """
    
    if model_names is None:
        model_names = ("Baseline", "Health-aware")
    
    baseline_name, health_name = model_names
    
    print(f"\n{'='*80}")
    print(f"ğŸ“Š Model Comparison: {baseline_name} vs {health_name}")
    print(f"{'='*80}\n")
    
    # ì„ í˜¸ë„ ì˜ˆì¸¡ ë©”íŠ¸ë¦­
    print("1ï¸âƒ£ Preference Prediction Metrics:")
    print(f"{'Metric':<20} {baseline_name:>15} {health_name:>15} {'Î”':>10}")
    print("-" * 65)
    
    preference_keys = ['accuracy', 'precision', 'recall', 'f1', 'auc']
    for key in preference_keys:
        baseline_val = baseline_metrics.get(key, 0)
        health_val = health_aware_metrics.get(key, 0)
        delta = health_val - baseline_val
        delta_str = f"{delta:+.4f}"
        
        print(f"{key.upper():<20} {baseline_val:>15.4f} {health_val:>15.4f} {delta_str:>10}")
    
    # ê±´ê°•ë„ ë©”íŠ¸ë¦­
    print(f"\n2ï¸âƒ£ Health-awareness Metrics:")
    print(f"{'Metric':<20} {baseline_name:>15} {health_name:>15} {'Î”':>10}")
    print("-" * 65)
    
    health_keys = ['avg_health_score', 'health_precision', 'health_improvement', 
                   'health_aware_recall']
    for key in health_keys:
        baseline_val = baseline_metrics.get(key, 0)
        health_val = health_aware_metrics.get(key, 0)
        delta = health_val - baseline_val
        delta_str = f"{delta:+.4f}"
        
        print(f"{key:<20} {baseline_val:>15.4f} {health_val:>15.4f} {delta_str:>10}")
    
    # ê· í˜• ë©”íŠ¸ë¦­
    print(f"\n3ï¸âƒ£ Balance Metrics:")
    print(f"{'Metric':<20} {baseline_name:>15} {health_name:>15} {'Î”':>10}")
    print("-" * 65)
    
    balance_keys = ['health_aware_f1', 'top_k_health', 'top_k_accuracy']
    for key in balance_keys:
        baseline_val = baseline_metrics.get(key, 0)
        health_val = health_aware_metrics.get(key, 0)
        delta = health_val - baseline_val
        delta_str = f"{delta:+.4f}"
        
        print(f"{key:<20} {baseline_val:>15.4f} {health_val:>15.4f} {delta_str:>10}")
    
    print(f"\n{'='*80}\n")
    
    # í•µì‹¬ ê²°ê³¼ ìš”ì•½
    print("ğŸ“Œ Key Findings:")
    
    # F1 Score ë¹„êµ
    f1_delta = health_aware_metrics['f1'] - baseline_metrics['f1']
    if f1_delta > 0.01:
        print(f"   âœ… F1 Score improved by {f1_delta:.2%}")
    elif f1_delta < -0.01:
        print(f"   âš ï¸  F1 Score decreased by {abs(f1_delta):.2%}")
    else:
        print(f"   â¡ï¸  F1 Score maintained ({f1_delta:+.2%})")
    
    # Health Score ë¹„êµ
    health_delta = health_aware_metrics['avg_health_score'] - baseline_metrics['avg_health_score']
    if health_delta > 0.05:
        print(f"   âœ… Average health score improved by {health_delta:.2%}")
    elif health_delta < -0.05:
        print(f"   âš ï¸  Average health score decreased by {abs(health_delta):.2%}")
    else:
        print(f"   â¡ï¸  Average health score maintained ({health_delta:+.2%})")
    
    # Health-aware F1
    ha_f1_baseline = baseline_metrics['health_aware_f1']
    ha_f1_health = health_aware_metrics['health_aware_f1']
    
    if ha_f1_health > ha_f1_baseline * 1.05:
        print(f"   âœ… Health-aware F1 improved by {(ha_f1_health/ha_f1_baseline - 1):.2%}")
    
    print(f"\n{'='*80}\n")


def compute_health_aware_ranking_metrics(predictions, targets, health_scores, k_list=[5, 10, 20]):
    """
    Ranking ê¸°ë°˜ ê±´ê°•ë„ ë©”íŠ¸ë¦­
    
    Args:
        predictions: ëª¨ë¸ì˜ ì¶”ì²œ í™•ë¥ 
        targets: ì‹¤ì œ ì„ í˜¸ë„
        health_scores: ê±´ê°• ì ìˆ˜
        k_list: Top-K ê°’ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        dict: Ranking ë©”íŠ¸ë¦­
    """
    
    predictions = predictions.cpu().detach().numpy()
    targets = targets.cpu().detach().numpy()
    health_scores = health_scores.cpu().detach().numpy()
    
    # ì˜ˆì¸¡ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_indices = np.argsort(predictions)[::-1]
    
    metrics = {}
    
    for k in k_list:
        k = min(k, len(predictions))
        top_k_indices = sorted_indices[:k]
        
        # Top-Kì˜ ì •í™•ë„
        top_k_accuracy = targets[top_k_indices].mean()
        
        # Top-Kì˜ í‰ê·  ê±´ê°• ì ìˆ˜
        top_k_health = health_scores[top_k_indices].mean()
        
        # NDCG@K (health scoreë¥¼ relevanceë¡œ ì‚¬ìš©)
        dcg = np.sum((2 ** health_scores[top_k_indices] - 1) / np.log2(np.arange(2, k + 2)))
        
        # Ideal DCG (ê±´ê°• ì ìˆ˜ ê¸°ì¤€ ì •ë ¬)
        ideal_indices = np.argsort(health_scores)[::-1][:k]
        idcg = np.sum((2 ** health_scores[ideal_indices] - 1) / np.log2(np.arange(2, k + 2)))
        
        ndcg = dcg / idcg if idcg > 0 else 0
        
        metrics[f'accuracy@{k}'] = top_k_accuracy
        metrics[f'health@{k}'] = top_k_health
        metrics[f'ndcg@{k}'] = ndcg
    
    return metrics


if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸ ì˜ˆì œ
    print("Testing evaluation metrics...")
    
    # ë”ë¯¸ ë°ì´í„°
    torch.manual_seed(42)
    predictions = torch.rand(100)
    targets = torch.randint(0, 2, (100,)).float()
    health_scores = torch.rand(100) * 0.5 + 0.3  # 0.3-0.8 ë²”ìœ„
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = compute_comprehensive_metrics(predictions, targets, health_scores)
    
    print("\nğŸ“Š Computed Metrics:")
    for key, value in metrics.items():
        print(f"   {key}: {value:.4f}")
    
    # Ranking ë©”íŠ¸ë¦­
    ranking_metrics = compute_health_aware_ranking_metrics(predictions, targets, health_scores)
    
    print("\nğŸ“ˆ Ranking Metrics:")
    for key, value in ranking_metrics.items():
        print(f"   {key}: {value:.4f}")
